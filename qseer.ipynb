{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGIK9O97vZRG",
        "outputId": "44d0ecdd-89f4-40cf-8af6-a58d20812c26"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# torchversion = torch.__version__\n",
        "\n",
        "# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-{torchversion}.html\n",
        "# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-{torchversion}.html\n",
        "# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkOtpaCIvro7",
        "outputId": "5e1d6ce4-8b39-43c7-ced7-b8b31d8d473d"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount(\"/content/gdrive\")\n",
        "# %cd /content/gdrive/MyDrive/QSeer/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "functools.partial(<function custom at 0x75b546083eb0>, optimizer=<function auto at 0x75b5d4155870>, memory_limit=None, debug_level=0)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import joblib\n",
        "import numpy as np\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.data import Data, InMemoryDataset\n",
        "import torch\n",
        "\n",
        "import os\n",
        "import networkx as nx\n",
        "import tensorcircuit as tc\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.nn import Linear, Sequential, ReLU#, Dropout, BatchNorm1d\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, GATConv, GINConv\n",
        "from torch_geometric.nn import global_mean_pool\n",
        "from torch_geometric.data import Batch, Dataset\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "K = tc.set_backend(\"numpy\") # solve JIT failed issue\n",
        "tc.set_contractor(\"auto\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The train data got error, so we just extract and process half the data from now on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_subset(file_path, ratio=0.5):\n",
        "    \"\"\"\n",
        "    Reads a dataset file, takes a portion of the data items according to ratio,\n",
        "    and saves to a new file with _{int(ratio*100)} suffix.\n",
        "    \"\"\"\n",
        "    # Load the full dataset\n",
        "    data_list = joblib.load(file_path)\n",
        "    \n",
        "    if not data_list:\n",
        "        print(\"No data found in the file.\")\n",
        "        return\n",
        "\n",
        "    # Calculate number of items to keep\n",
        "    n_keep = max(1, int(len(data_list) * ratio))\n",
        "    subset_list = data_list[:n_keep]\n",
        "\n",
        "    # Create new file name\n",
        "    dir_name, base_name = os.path.split(file_path)\n",
        "    name, ext = os.path.splitext(base_name)\n",
        "    new_file_name = os.path.join(dir_name, f\"{name}_{int(ratio*100)}{ext}\")\n",
        "\n",
        "    # Save the subset\n",
        "    joblib.dump(subset_list, new_file_name)\n",
        "    print(f\"Saved subset ({len(subset_list)} items) to {new_file_name}\")\n",
        "\n",
        "# Example usage\n",
        "# create_subset('data/graph_train.pkl', ratio=0.5)\n",
        "# create_subset('data/graph_test.pkl', ratio=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9tgxGRRwCe_",
        "outputId": "66cfee19-843b-4203-c104-ab27a0fd79d7"
      },
      "outputs": [],
      "source": [
        "def interleave_arrays(arr1, arr2):\n",
        "  return np.ravel(np.column_stack((arr1, arr2)))\n",
        "\n",
        "class GraphDataset(InMemoryDataset):\n",
        "  def __init__(self, root, file_name=None, transform=None, pre_transform=None, pre_filter=None):\n",
        "    self.filename = file_name\n",
        "    super(GraphDataset, self).__init__(root, transform, pre_transform, pre_filter)\n",
        "    self.load(self.processed_paths[0])\n",
        "\n",
        "  @property\n",
        "  def raw_file_names(self):\n",
        "    return [self.filename]\n",
        "\n",
        "  @property\n",
        "  def processed_file_names(self):\n",
        "    return [self.filename + \".pt\"]\n",
        "\n",
        "  def process(self):\n",
        "      data_list = []\n",
        "      my_list = joblib.load(self.filename)\n",
        "\n",
        "      for item in my_list:\n",
        "          graph = item[\"graph\"]\n",
        "          gamma = np.array(item[\"gamma\"], dtype=np.float32)\n",
        "          beta = np.array(item[\"beta\"], dtype=np.float32)\n",
        "\n",
        "          # Layer\n",
        "          layer_val = np.shape(gamma)[0]\n",
        "          n_values = 4\n",
        "          layer = np.eye(n_values)[layer_val - 1]\n",
        "\n",
        "          # Label\n",
        "          label = interleave_arrays(gamma, beta)\n",
        "          label = np.pad(label, (0, 8 - label.shape[0]), mode='constant')\n",
        "\n",
        "          # Edge index\n",
        "          edge_index = torch.tensor(list(graph.edges())).t().contiguous()\n",
        "\n",
        "          # Edge attr and node features\n",
        "          edge_attr = torch.arange(graph.number_of_edges(), dtype=torch.float32).unsqueeze(1)\n",
        "          x = torch.arange(graph.number_of_nodes(), dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "          # Labels and layers as tensors\n",
        "          y = torch.tensor(label, dtype=torch.float32).unsqueeze(0)\n",
        "          layer_tensor = torch.tensor(layer, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "          data = Data(edge_index=edge_index, edge_attr=edge_attr, x=x, y=y, layer=layer_tensor)\n",
        "          data_list.append(data)\n",
        "\n",
        "      self.save(data_list, self.processed_paths[0])\n",
        "\n",
        "# train_dataset = GraphDataset(root='./data/', file_name='./data/graph_train_50.pkl')\n",
        "# test_dataset = GraphDataset(root='./data/', file_name='./data/graph_test.pkl')\n",
        "# valid_dataset = GraphDataset(root='./data/', file_name='./data/graph_valid.pkl')\n",
        "\n",
        "# print(train_dataset.num_node_features)\n",
        "# print(train_dataset.num_classes)\n",
        "# print(train_dataset[0].layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check sample data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "my_list = joblib.load(\"data/graph_test.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== First 3 items in dataset ===\")\n",
        "for i, item in enumerate(my_list[:3]):\n",
        "    print(f\"\\nItem {i}:\")\n",
        "    print(\"  graph:\", item[\"graph\"])\n",
        "    print(\"  gamma:\", item[\"gamma\"])\n",
        "    print(\"  beta :\", item[\"beta\"])\n",
        "    print(\"  c    :\", item[\"c\"])\n",
        "    print(\"  ci   :\", item[\"ci\"])\n",
        "\n",
        "# Work on the first graph\n",
        "graph = my_list[1][\"graph\"]\n",
        "print(\"\\n=== Selected Graph (Item 0) ===\")\n",
        "print(\"Number of nodes:\", graph.number_of_nodes())\n",
        "print(\"Number of edges:\", graph.number_of_edges())\n",
        "print(\"Edges:\", list(graph.edges()))\n",
        "\n",
        "# Edge index\n",
        "edge_index = torch.tensor(list(graph.edges())).t().contiguous()\n",
        "print(\"\\nEdge Index Tensor:\")\n",
        "print(edge_index)\n",
        "print(\"Shape:\", edge_index.shape)\n",
        "\n",
        "# Edge attr\n",
        "edge_attr = torch.arange(graph.number_of_edges(), dtype=torch.float32).unsqueeze(1)\n",
        "print(\"\\nEdge Attr Tensor:\")\n",
        "print(edge_attr)\n",
        "print(\"Shape:\", edge_attr.shape)\n",
        "\n",
        "# Node features\n",
        "x = torch.arange(graph.number_of_nodes(), dtype=torch.float32).unsqueeze(1)\n",
        "print(\"\\nNode Feature Tensor (x):\")\n",
        "print(x)\n",
        "print(\"Shape:\", x.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKDaU9rtxFAV",
        "outputId": "ce025531-b1a1-43ce-8cdc-ac6e7b8a45c4"
      },
      "outputs": [],
      "source": [
        "def interleave_arrays(arr1, arr2):\n",
        "  return np.ravel(np.column_stack((arr1, arr2)))\n",
        "\n",
        "class GraphWDataset(InMemoryDataset):\n",
        "  def __init__(self, root, file_name=None, transform=None, pre_transform=None, pre_filter=None):\n",
        "    self.filename = file_name\n",
        "    super(GraphWDataset, self).__init__(root, transform, pre_transform, pre_filter)\n",
        "    self.load(self.processed_paths[0])\n",
        "\n",
        "  @property\n",
        "  def raw_file_names(self):\n",
        "    return [self.filename]\n",
        "\n",
        "  @property\n",
        "  def processed_file_names(self):\n",
        "    return [self.filename + \".pt\"]\n",
        "\n",
        "  def process(self):\n",
        "    data_list = []\n",
        "    my_list = joblib.load(self.filename)\n",
        "    index = 0\n",
        "    for item in my_list:\n",
        "      source_list = item[\"slist\"]\n",
        "      target_list = item[\"tlist\"]\n",
        "\n",
        "      gamma = np.array(item[\"gamma\"], dtype=np.float32)\n",
        "      beta =  np.array(item[\"beta\"], dtype=np.float32)\n",
        "      layer = np.array(np.shape(gamma)[0], dtype=np.int32)\n",
        "      n_values = np.max(4)\n",
        "      layer = np.eye(n_values)[layer-1]\n",
        "      label = interleave_arrays(gamma, beta)\n",
        "      label = np.pad(label, (0, 8 - np.shape(label)[0]), mode='constant', constant_values=0)\n",
        "\n",
        "      edge_index = torch.tensor([source_list, target_list], dtype=torch.int64)\n",
        "      edge_attr = torch.tensor([[item[\"wlist\"][i]] for i in range(len(item[\"wlist\"]))], dtype=torch.float32)\n",
        "      num_nodes = torch.max(edge_index).item() + 1\n",
        "      x = torch.tensor([[i] for i in range(num_nodes)], dtype=torch.float32)\n",
        "      y = torch.tensor(label, dtype=torch.float32).flatten()\n",
        "      y = torch.unsqueeze(y, 0)\n",
        "      layer = torch.tensor(layer, dtype=torch.float32).flatten()\n",
        "      layer = torch.unsqueeze(layer, 0)\n",
        "      data = Data(edge_index=edge_index, edge_attr=edge_attr, x=x, y=y, layer=layer)\n",
        "      data_list.append(data)\n",
        "\n",
        "    self.save(data_list, self.processed_paths[0])\n",
        "\n",
        "\n",
        "# train_dataset = GraphWDataset(root='./data/', file_name='./data/weighted_graph_train.pkl')\n",
        "# test_dataset = GraphWDataset(root='./data/', file_name='./data/weighted_graph_test.pkl')\n",
        "# valid_dataset = GraphWDataset(root='./data/', file_name='./data/weighted_graph_valid.pkl')\n",
        "\n",
        "# print(train_dataset.num_node_features)\n",
        "# print(train_dataset.num_classes)\n",
        "# print(train_dataset[0].layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujxIWSiJ_oHw"
      },
      "outputs": [],
      "source": [
        "\n",
        "def interleave_arrays(arr1, arr2):\n",
        "  return np.ravel(np.column_stack((arr1, arr2)))\n",
        "\n",
        "class GraphGDataset(InMemoryDataset):\n",
        "  def __init__(self, root, file_name=None, num_layer = 4, transform=None, pre_transform=None, pre_filter=None):\n",
        "    self.filename = file_name\n",
        "    self.num_layer = num_layer\n",
        "    super(GraphGDataset, self).__init__(root, transform, pre_transform, pre_filter)\n",
        "    self.load(self.processed_paths[0])\n",
        "\n",
        "  @property\n",
        "  def raw_file_names(self):\n",
        "    return [self.filename]\n",
        "\n",
        "  @property\n",
        "  def processed_file_names(self):\n",
        "    return [self.filename + \"_\" + str(self.num_layer) + \"_.pt\"]\n",
        "\n",
        "  def process(self):\n",
        "    data_list = []\n",
        "    graph_list = joblib.load(self.filename)\n",
        "    index = 0\n",
        "    for graph in graph_list:\n",
        "      source_list = None\n",
        "      target_list = None\n",
        "      edge_list = graph.edges()\n",
        "      for edge in edge_list:\n",
        "        if source_list is None:\n",
        "          source_list = np.array([edge[0]])\n",
        "          target_list = np.array([edge[1]])\n",
        "        else:\n",
        "          source_list = np.append(source_list, edge[0])\n",
        "          target_list = np.append(target_list, edge[1])\n",
        "\n",
        "      layer = np.array(self.num_layer, dtype=np.int32)\n",
        "      n_values = np.max(4)\n",
        "      layer = np.eye(n_values)[layer-1]\n",
        "\n",
        "      edge_index = torch.tensor([source_list, target_list], dtype=torch.int64)\n",
        "      edge_attr = torch.tensor([[i] for i in range(len(edge_list))], dtype=torch.float32)\n",
        "\n",
        "      x = torch.tensor([[i] for i in range(graph.number_of_nodes())], dtype=torch.float32)\n",
        "      layer = torch.tensor(layer, dtype=torch.float32).flatten()\n",
        "      layer = torch.unsqueeze(layer, 0)\n",
        "      data = Data(edge_index=edge_index, edge_attr=edge_attr, x=x, y=None, layer=layer)\n",
        "      data_list.append(data)\n",
        "\n",
        "    self.save(data_list, self.processed_paths[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEbkS5-GxEOP"
      },
      "outputs": [],
      "source": [
        "class DGNN(torch.nn.Module):\n",
        "  def __init__(self, n_x, n_y, dim_h):\n",
        "    super(DGNN, self).__init__()\n",
        "    hidden_channels_gcn = dim_h * 2\n",
        "    hidden_channels_gat = hidden_channels_gcn * 2\n",
        "    hidden_channels_gin = hidden_channels_gat * 2\n",
        "\n",
        "    # Initialize the first GCNConv layer in the forward method\n",
        "    self.conv1 = GCNConv(n_x, hidden_channels_gcn)\n",
        "    self.hidden_channels_gcn = hidden_channels_gcn\n",
        "    self.conv2 = GCNConv(hidden_channels_gcn, hidden_channels_gcn)\n",
        "    self.gat_conv1 = GATConv(hidden_channels_gcn, hidden_channels_gat)\n",
        "    self.gat_conv2 = GATConv(hidden_channels_gat, hidden_channels_gat)\n",
        "\n",
        "    mlp = torch.nn.Sequential(\n",
        "        Linear(hidden_channels_gat, hidden_channels_gin),\n",
        "        ReLU(),\n",
        "        Linear(hidden_channels_gin, hidden_channels_gin)\n",
        "    )\n",
        "    self.gin_conv1 = GINConv(mlp)\n",
        "    self.out = Linear(hidden_channels_gin + 4, hidden_channels_gin)\n",
        "    self.out2 = Linear(hidden_channels_gin, n_y)\n",
        "\n",
        "  def forward(self, x, edge_index, edge_weight, layer, batch):\n",
        "    x = F.relu(self.conv1(x = x, edge_index = edge_index, edge_weight = edge_weight))\n",
        "    #print(x)\n",
        "    x = F.relu(self.conv2(x = x, edge_index = edge_index, edge_weight = edge_weight))\n",
        "    x = F.relu(self.gat_conv1(x = x, edge_index = edge_index, edge_attr = edge_weight))\n",
        "    x = F.relu(self.gat_conv2(x = x, edge_index = edge_index, edge_attr = edge_weight))\n",
        "    x = F.relu(self.gin_conv1(x = x, edge_index = edge_index))\n",
        "    x = global_mean_pool(x, batch)\n",
        "    #print(x.shape)\n",
        "    #print(layer.shape)\n",
        "    x = torch.cat([x, layer], dim=1)\n",
        "    x = self.out(x)\n",
        "    x = self.out2(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nb1Xe77dyIDT",
        "outputId": "53a1a9ea-46f7-4f80-8fb5-d8ff361de596"
      },
      "outputs": [],
      "source": [
        "train_dataset = GraphDataset(root='./data/', file_name='./data/graph_train_50.pkl')\n",
        "test_dataset = GraphDataset(root='./data/', file_name='./data/graph_test.pkl')\n",
        "valid_dataset = GraphDataset(root='./data/', file_name='./data/graph_valid.pkl')\n",
        "\n",
        "model = DGNN(n_x=train_dataset.num_node_features, n_y=train_dataset.num_classes, dim_h=256).to(\"cuda\")\n",
        "loss_fn = nn.MSELoss()  # mean square error\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "n_epochs = 3\n",
        "batch_size = 4096\n",
        "myloader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "youloader = DataLoader(valid_dataset, batch_size=batch_size)\n",
        "testloader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "save_path = './best_model.pth'\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  model.train()\n",
        "  mse_list = []\n",
        "  for data in myloader:\n",
        "    data = data.to(\"cuda\")\n",
        "    out = model(data.x, data.edge_index, data.edge_attr, data.layer, data.batch)\n",
        "    loss = loss_fn(out, data.y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    mse_list.append(float(loss))\n",
        "  train_mse = sum(mse_list) / len(mse_list)\n",
        "  print(f\"model epoch {epoch} training mse {train_mse:.4f}\")\n",
        "\n",
        "  model.eval()\n",
        "  mse_list = []\n",
        "  for data in youloader:\n",
        "    data = data.to(\"cuda\")\n",
        "    out = model(data.x, data.edge_index, data.edge_attr, data.layer, data.batch)\n",
        "    loss = loss_fn(out, data.y)\n",
        "    mse_list.append(float(loss))\n",
        "  valid_mse = sum(mse_list) / len(mse_list)\n",
        "  print(f\"model epoch {epoch} valid mse {valid_mse:.4f}\")\n",
        "\n",
        "  if valid_mse < best_val_loss:\n",
        "    best_val_loss = valid_mse\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f'Saved best model with validation loss: {best_val_loss:.4f}')\n",
        "\n",
        "  scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ggk3hVjZ1mrJ",
        "outputId": "59e261ff-27e7-4f1c-cb42-5e70b34e5c5f"
      },
      "outputs": [],
      "source": [
        "train_dataset = GraphWDataset(root='./data/', file_name='./data/weighted_graph_train.pkl')\n",
        "test_dataset = GraphWDataset(root='./data/', file_name='./data/weighted_graph_test.pkl')\n",
        "valid_dataset = GraphWDataset(root='./data/', file_name='./data/weighted_graph_valid.pkl')\n",
        "\n",
        "model = DGNN(n_x=train_dataset.num_node_features, n_y=train_dataset.num_classes, dim_h=256).to(\"cuda\")\n",
        "#model.load_state_dict(torch.load(save_path, weights_only=True))\n",
        "\n",
        "loss_fn = nn.MSELoss()  # mean square error\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "n_epochs = 3\n",
        "batch_size = 1024\n",
        "myloader = DataLoader(train_dataset, batch_size=batch_size)\n",
        "youloader = DataLoader(valid_dataset, batch_size=batch_size)\n",
        "testloader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "save_path = './best_weighted_model.pth'\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  model.train()\n",
        "  mse_list = []\n",
        "  for data in myloader:\n",
        "    data = data.to(\"cuda\")\n",
        "    out = model(data.x, data.edge_index, data.edge_attr, data.layer, data.batch)\n",
        "    loss = loss_fn(out, data.y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    mse_list.append(float(loss))\n",
        "  train_mse = sum(mse_list) / len(mse_list)\n",
        "  print(f\"model epoch {epoch} training mse {train_mse:.4f}\")\n",
        "\n",
        "  model.eval()\n",
        "  mse_list = []\n",
        "  for data in youloader:\n",
        "    data = data.to(\"cuda\")\n",
        "    out = model(data.x, data.edge_index, data.edge_attr, data.layer, data.batch)\n",
        "    loss = loss_fn(out, data.y)\n",
        "    mse_list.append(float(loss))\n",
        "  valid_mse = sum(mse_list) / len(mse_list)\n",
        "  print(f\"model epoch {epoch} valid mse {valid_mse:.4f}\")\n",
        "\n",
        "  if valid_mse < best_val_loss:\n",
        "    best_val_loss = valid_mse\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "    print(f'Saved best model with validation loss: {best_val_loss:.4f}')\n",
        "\n",
        "  scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfADDu77-vRj",
        "outputId": "ab614c45-907d-4da4-fbc8-4817bf392cba"
      },
      "outputs": [],
      "source": [
        "save_path = './best_model.pth'\n",
        "test_dataset = GraphDataset(root='./data/', file_name='./data/graph_test.pkl')\n",
        "model = DGNN(n_x=test_dataset.num_node_features, n_y=test_dataset.num_classes, dim_h=256).to(\"cuda\")\n",
        "model.load_state_dict(torch.load(save_path, weights_only=True))\n",
        "model.eval()\n",
        "testloader = DataLoader(test_dataset, batch_size=4096)\n",
        "test_result = []\n",
        "for data in testloader:\n",
        "  data = data.to(\"cuda\")\n",
        "  out = model(data.x, data.edge_index, data.edge_attr, data.layer, data.batch)\n",
        "  test_result.append(out.cpu().detach().numpy())\n",
        "\n",
        "test_result = np.concatenate(test_result, axis=0)\n",
        "print(np.shape(test_result))\n",
        "joblib.dump(test_result, \"./test_result.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPWD7dc9-3mF",
        "outputId": "de163279-7b70-4ea5-cff8-47488914f7e8"
      },
      "outputs": [],
      "source": [
        "save_path = './best_weighted_model.pth'\n",
        "test_dataset = GraphWDataset(root='./data/', file_name='./data/weighted_graph_test.pkl')\n",
        "model = DGNN(n_x=test_dataset.num_node_features, n_y=test_dataset.num_classes, dim_h=256).to(\"cuda\")\n",
        "model.load_state_dict(torch.load(save_path, weights_only=True))\n",
        "model.eval()\n",
        "testloader = DataLoader(test_dataset, batch_size=4096)\n",
        "test_result = []\n",
        "for data in testloader:\n",
        "  data = data.to(\"cuda\")\n",
        "  out = model(data.x, data.edge_index, data.edge_attr, data.layer, data.batch)\n",
        "  test_result.append(out.cpu().detach().numpy())\n",
        "\n",
        "test_result = np.concatenate(test_result, axis=0)\n",
        "print(np.shape(test_result))\n",
        "joblib.dump(test_result, \"./test_result_weighted.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzVr0QHv_ZZ3",
        "outputId": "14c77b73-3abf-43f9-8faf-cfb135807753"
      },
      "outputs": [],
      "source": [
        "save_path = './best_model.pth'\n",
        "test_dataset1 = GraphGDataset(root='./data/', file_name='./data/graph_unseen_test.pkl', num_layer=1)\n",
        "test_dataset2 = GraphGDataset(root='./data/', file_name='./data/graph_unseen_test.pkl', num_layer=2)\n",
        "test_dataset3 = GraphGDataset(root='./data/', file_name='./data/graph_unseen_test.pkl', num_layer=3)\n",
        "\n",
        "\n",
        "model = DGNN(n_x=1, n_y=8, dim_h=256).to(\"cuda\")\n",
        "model.load_state_dict(torch.load(save_path, weights_only=True))\n",
        "model.eval()\n",
        "\n",
        "testloader = DataLoader(test_dataset1, batch_size=250)\n",
        "\n",
        "print(len(testloader))\n",
        "\n",
        "test_result = []\n",
        "for data in testloader:\n",
        "  data = data.to(\"cuda\")\n",
        "  out = model(data.x, data.edge_index, data.edge_attr, data.layer, data.batch)\n",
        "  test_result.append(out.cpu().detach().numpy())\n",
        "test_result = np.concatenate(test_result, axis=0)\n",
        "print(np.shape(test_result))\n",
        "joblib.dump(test_result, f\"./unseen_result_1.pkl\")\n",
        "\n",
        "\n",
        "testloader = DataLoader(test_dataset2, batch_size=250)\n",
        "test_result = []\n",
        "for data in testloader:\n",
        "  data = data.to(\"cuda\")\n",
        "  out = model(data.x, data.edge_index, data.edge_attr, data.layer, data.batch)\n",
        "  test_result.append(out.cpu().detach().numpy())\n",
        "test_result = np.concatenate(test_result, axis=0)\n",
        "print(np.shape(test_result))\n",
        "joblib.dump(test_result, f\"./unseen_result_2.pkl\")\n",
        "\n",
        "testloader = DataLoader(test_dataset3, batch_size=250)\n",
        "test_result = []\n",
        "for data in testloader:\n",
        "  data = data.to(\"cuda\")\n",
        "  out = model(data.x, data.edge_index, data.edge_attr, data.layer, data.batch)\n",
        "  test_result.append(out.cpu().detach().numpy())\n",
        "test_result = np.concatenate(test_result, axis=0)\n",
        "print(np.shape(test_result))\n",
        "joblib.dump(test_result, f\"./unseen_result_3.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Quantum comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def interleave_arrays(arr1, arr2):\n",
        "  return np.ravel(np.column_stack((arr1, arr2)))\n",
        "\n",
        "def beta_to_qaoa(beta):\n",
        "  return np.pi * np.array(beta, dtype=np.float32)\n",
        "\n",
        "def gamma_to_qaoa(gamma):\n",
        "  return -np.pi * np.array(gamma, dtype=np.float32) / 2\n",
        "\n",
        "def all_to_qaoa(param, layer):\n",
        "  pi0 = [np.pi] * layer\n",
        "  pi1 = [-np.pi/2] * layer\n",
        "  pi2 = interleave_arrays(pi1, pi0)\n",
        "  result = pi2 * param\n",
        "  return result.astype(np.float32)\n",
        "\n",
        "def QAOAansatz(params, g=None, L=4, return_circuit=False):\n",
        "    n_qubits = len(g.nodes)\n",
        "    circuit = tc.Circuit(n_qubits)\n",
        "    for qubit in range(n_qubits):\n",
        "        circuit.H(qubit)\n",
        "\n",
        "    for layer_index in range(L):\n",
        "        for edge in g.edges:\n",
        "            circuit.exp1(edge[0], edge[1], unitary=tc.gates._zz_matrix, theta=params[2 * layer_index])\n",
        "        for qubit in range(n_qubits):\n",
        "            circuit.rx(qubit, theta=params[2 * layer_index + 1])\n",
        "\n",
        "    if return_circuit:\n",
        "        return circuit\n",
        "\n",
        "    energy = 0.0\n",
        "    for edge in g.edges:\n",
        "        energy += circuit.expectation_ps(z=[edge[0], edge[1]])\n",
        "\n",
        "    return K.real(energy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total graphs: 81958\n",
            "Testing on: 50 graphs\n",
            "Batch size: 10\n",
            "\n",
            "Processing batch: 0 to 10\n",
            "Processed: 10/50\n",
            "  Better than random: 2/10 (20.0%)\n",
            "  Better than ground truth: 2/10 (20.0%)\n",
            "\n",
            "Processing batch: 10 to 20\n",
            "Processed: 20/50\n",
            "  Better than random: 5/20 (25.0%)\n",
            "  Better than ground truth: 2/20 (10.0%)\n",
            "\n",
            "Processing batch: 20 to 30\n",
            "Processed: 30/50\n",
            "  Better than random: 5/30 (16.7%)\n",
            "  Better than ground truth: 3/30 (10.0%)\n",
            "\n",
            "Processing batch: 30 to 40\n",
            "Processed: 40/50\n",
            "  Better than random: 7/40 (17.5%)\n",
            "  Better than ground truth: 4/40 (10.0%)\n",
            "\n",
            "Processing batch: 40 to 50\n",
            "Processed: 50/50\n",
            "  Better than random: 8/50 (16.0%)\n",
            "  Better than ground truth: 4/50 (8.0%)\n",
            "\n",
            "==================================================\n",
            "FINAL RESULTS\n",
            "==================================================\n",
            "Total tested: 50\n",
            "Better than random: 8 (16.0%)\n",
            "Better than ground truth: 4 (8.0%)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load data\n",
        "graph_test = joblib.load(\"./data/graph_test.pkl\")\n",
        "result_test = joblib.load(\"./test_result.pkl\")\n",
        "\n",
        "# SAMPLING PARAMETERS\n",
        "SAMPLE_SIZE = 50\n",
        "BATCH_SIZE = 10\n",
        "\n",
        "print(f\"Total graphs: {len(graph_test)}\")\n",
        "print(f\"Testing on: {SAMPLE_SIZE} graphs\")\n",
        "print(f\"Batch size: {BATCH_SIZE}\")\n",
        "\n",
        "graph_index = 0\n",
        "prediction_better_than_random = 0\n",
        "prediction_better_than_ground_truth = 0\n",
        "\n",
        "for batch_start in range(0, min(SAMPLE_SIZE, len(graph_test)), BATCH_SIZE):\n",
        "    batch_end = min(batch_start + BATCH_SIZE, SAMPLE_SIZE, len(graph_test))\n",
        "    print(f\"\\nProcessing batch: {batch_start} to {batch_end}\")\n",
        "\n",
        "    for i in range(batch_start, batch_end):\n",
        "        graph_data = graph_test[i]\n",
        "        g = graph_data[\"graph\"]\n",
        "        gamma_angles = gamma_to_qaoa(graph_data[\"gamma\"])\n",
        "        beta_angles = beta_to_qaoa(graph_data[\"beta\"])\n",
        "        num_layers = np.shape(gamma_angles)[0]\n",
        "\n",
        "        # Prepare parameters\n",
        "        ground_truth_params = interleave_arrays(gamma_angles, beta_angles)\n",
        "        ground_truth_params = np.pad(ground_truth_params, (0, 8 - np.shape(ground_truth_params)[0]),\n",
        "                                     mode='constant', constant_values=0)\n",
        "\n",
        "        predicted_params = result_test[graph_index]\n",
        "        predicted_params = all_to_qaoa(predicted_params, 4)\n",
        "\n",
        "        # Three parameter sets\n",
        "        random_params = np.random.randn(8) * 0.1\n",
        "        parameter_sets = np.stack([random_params, ground_truth_params, predicted_params], axis=0)\n",
        "\n",
        "        # Evaluate energies\n",
        "        energies = []\n",
        "        for circuit_index in range(len(parameter_sets)):\n",
        "            energy = QAOAansatz(params=parameter_sets[circuit_index], g=g, L=num_layers)\n",
        "            energies.append(float(energy))\n",
        "\n",
        "        # Compare results\n",
        "        if energies[2] < energies[0]:\n",
        "            prediction_better_than_random += 1\n",
        "        if energies[2] < energies[1]:\n",
        "            prediction_better_than_ground_truth += 1\n",
        "\n",
        "        graph_index += 1\n",
        "\n",
        "        if graph_index % 10 == 0:\n",
        "            print(f\"Processed: {graph_index}/{SAMPLE_SIZE}\")\n",
        "            print(f\"  Better than random: {prediction_better_than_random}/{graph_index} \"\n",
        "                  f\"({100 * prediction_better_than_random / graph_index:.1f}%)\")\n",
        "            print(f\"  Better than ground truth: {prediction_better_than_ground_truth}/{graph_index} \"\n",
        "                  f\"({100 * prediction_better_than_ground_truth / graph_index:.1f}%)\")\n",
        "\n",
        "# Final results\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"FINAL RESULTS\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Total tested: {graph_index}\")\n",
        "print(f\"Better than random: {prediction_better_than_random} \"\n",
        "      f\"({100 * prediction_better_than_random / graph_index:.1f}%)\")\n",
        "print(f\"Better than ground truth: {prediction_better_than_ground_truth} \"\n",
        "      f\"({100 * prediction_better_than_ground_truth / graph_index:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[-1.2394050732254982, -2.0816769301891327, 0.9993368089199066]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "energies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOP5LJ3TAj77",
        "outputId": "0c9c56ac-f144-49be-cafa-a7e8670370b6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# QAOA_vvag = K.jit(\n",
        "#   tc.backend.vvag(QAOAansatz, argnums=0, vectorized_argnums=0), static_argnums=(1, 3)\n",
        "# )\n",
        "# opt = K.optimizer(tf.keras.optimizers.Adam(1e-3))\n",
        "\n",
        "# graph_test = joblib.load(\"./data/graph_test.pkl\")\n",
        "# result_test = joblib.load(\"./test_result.pkl\")\n",
        "\n",
        "# index = 0\n",
        "# better = 0\n",
        "# best = 0\n",
        "\n",
        "# for i, graph in enumerate(graph_test):\n",
        "#   g = graph[\"graph\"]\n",
        "#   gamma = gamma_to_qaoa(graph[\"gamma\"])\n",
        "#   beta =  beta_to_qaoa(graph[\"beta\"])\n",
        "#   layer = np.shape(gamma)[0]\n",
        "#   label = interleave_arrays(gamma, beta)\n",
        "#   label = np.pad(label, (0, 8 - np.shape(label)[0]), mode='constant', constant_values=0)\n",
        "#   label = np.expand_dims(label, axis=0)\n",
        "#   para = result_test[index]\n",
        "#   para = all_to_qaoa(para, 4)\n",
        "#   para = np.expand_dims(para, axis=0)\n",
        "#   p1 = K.implicit_randn(shape=[1, 8], stddev=0.1) # random\n",
        "#   p2 = tf.convert_to_tensor(label) # ground truth\n",
        "#   p3 = tf.convert_to_tensor(para) # predict\n",
        "#   p = tf.concat([p1, p2, p3], 0)\n",
        "\n",
        "#   LL = []\n",
        "#   for num_circuit in range(3):\n",
        "#     #c = QAOAansatz(params=p[num_circuit], g=prob, return_circuit=True)\n",
        "#     loss = QAOAansatz(params=p[num_circuit], g=g, L=layer)\n",
        "#     LL.append(K.numpy(loss))\n",
        "\n",
        "#   if LL[2] < LL[0]:\n",
        "#     better += 1\n",
        "\n",
        "#   if LL[2] < LL[1]:\n",
        "#     best += 1\n",
        "\n",
        "#   index += 1\n",
        "#   if index % 100 == 0:\n",
        "#     print(index)\n",
        "#     print(better)\n",
        "#     print(best)\n",
        "\n",
        "\n",
        "# print(index)\n",
        "# print(better)\n",
        "# print(best)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "qseer",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
